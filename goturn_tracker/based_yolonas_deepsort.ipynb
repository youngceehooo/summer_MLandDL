{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5243e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import cv2\n",
    "import torch\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de5a805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x25e90edd2b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define command line flags\n",
    "flags.DEFINE_string('f', 'value', 'The explanation of this parameter')\n",
    "flags.DEFINE_string('model', 'yolo_nas_l', 'yolo_nas_l or yolo_nas_m or yolo_nas_s')\n",
    "flags.DEFINE_string('video', \"16run.mp4\", 'path to input video or set to 0 for webcam')\n",
    "flags.DEFINE_string('output', \"output.mp4\", 'path to output video')\n",
    "flags.DEFINE_float('conf', 0.50, 'confidence threshhold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71da25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_argv):\n",
    "    # Initialize the video capture and the video writer objects\n",
    "    video_cap = cv2.VideoCapture(FLAGS.video)\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Initialize the video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    writer = cv2.VideoWriter(FLAGS.output, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize the DeepSort tracker\n",
    "    tracker = DeepSort(max_age=50)\n",
    "\n",
    "    # Check if GPU is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Load the YOLO model\n",
    "    model = models.get(FLAGS.model, pretrained_weights=\"coco\").to(device)\n",
    "\n",
    "    # Load the COCO class labels the YOLO model was trained on\n",
    "    classes_path = \"C:\\\\Users\\\\obvious\\\\Desktop\\\\sight\\\\coco.names\"\n",
    "    with open(classes_path, \"r\") as f:\n",
    "        class_names = f.read().strip().split(\"\\n\")\n",
    "\n",
    "    # Create a list of random colors to represent each class\n",
    "    np.random.seed(42)  # to get the same colors\n",
    "    colors = np.random.randint(0, 255, size=(len(class_names), 3))  # (80, 3)\n",
    "    while True:\n",
    "        # Start time to compute the FPS\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_cap.read()\n",
    "\n",
    "        # If there is no frame, we have reached the end of the video\n",
    "        if not ret:\n",
    "            print(\"End of the video file...\")\n",
    "            break\n",
    "\n",
    "        # Run the YOLO model on the frame\n",
    "\n",
    "        # Perform object detection using the YOLO model on the current frame\n",
    "        detect = next(iter(model.predict(frame, iou=0.5, conf=FLAGS.conf)))\n",
    "\n",
    "        # Extract the bounding box coordinates, confidence scores, and class labels from the detection results\n",
    "        bboxes_xyxy = torch.from_numpy(detect.prediction.bboxes_xyxy).tolist()\n",
    "        confidence = torch.from_numpy(detect.prediction.confidence).tolist()\n",
    "        labels = torch.from_numpy(detect.prediction.labels).tolist()\n",
    "        # Combine the bounding box coordinates and confidence scores into a single list\n",
    "        concate = [sublist + [element] for sublist, element in zip(bboxes_xyxy, confidence)]\n",
    "        # Combine the concatenated list with the class labels into a final prediction list\n",
    "        final_prediction = [sublist + [element] for sublist, element in zip(concate, labels)]\n",
    "\n",
    "        # Initialize the list of bounding boxes and confidences\n",
    "        results = []\n",
    "                # Loop over the detections\n",
    "        for data in final_prediction:\n",
    "            # Extract the confidence (i.e., probability) associated with the detection\n",
    "            confidence = data[4]\n",
    "\n",
    "            # Filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "            if float(confidence) < FLAGS.conf:\n",
    "                continue\n",
    "\n",
    "            # If the confidence is greater than the minimum confidence, draw the bounding box on the frame\n",
    "            xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "            class_id = int(data[5])\n",
    "            \n",
    "            # Add the bounding box (x, y, w, h), confidence, and class ID to the results list\n",
    "            results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "        # Update the tracker with the new detections\n",
    "        tracks = tracker.update_tracks(results, frame=frame)\n",
    "                # Loop over the tracks\n",
    "        for track in tracks:\n",
    "            # If the track is not confirmed, ignore it\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            # Get the track ID and the bounding box\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            class_id = track.get_det_class()\n",
    "            x1, y1, x2, y2 = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "            \n",
    "            # Get the color for the class\n",
    "            color = colors[class_id]\n",
    "            B, G, R = int(color[0]), int(color[1]), int(color[2])\n",
    "            \n",
    "            # Create text for track ID and class name\n",
    "            text = str(track_id) + \" - \" + str(class_names[class_id])\n",
    "            \n",
    "            # Draw bounding box and text on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (B, G, R), 2)\n",
    "            cv2.rectangle(frame, (x1 - 1, y1 - 20), (x1 + len(text) * 12, y1), (B, G, R), -1)\n",
    "            cv2.putText(frame, text, (x1 + 5, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        # End time to compute the FPS\n",
    "        end = datetime.datetime.now()\n",
    "                # Show the time it took to process 1 frame\n",
    "        print(f\"Time to process 1 frame: {(end - start).total_seconds() * 1000:.0f} milliseconds\")\n",
    "        \n",
    "        # Calculate the frames per second and draw it on the frame\n",
    "        fps = f\"FPS: {1 / (end - start).total_seconds():.2f}\"\n",
    "        cv2.putText(frame, fps, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 8)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        # Write the frame to the output video file\n",
    "        writer.write(frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit the loop\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release video capture and video writer objects\n",
    "    video_cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cbbcddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://sghub.deci.ai/models/yolo_nas_l_coco.pth\" to C:\\Users\\obvious/.cache\\torch\\hub\\checkpoints\\yolo_nas_l_coco.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 256M/256M [00:16<00:00, 16.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(main)\n",
    "    except SystemExit:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f566ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.8.17",
   "language": "python",
   "name": "pytorch3.8.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
