{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbe489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import cv2\n",
    "import torch\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "# Define command line flags\n",
    "flags.DEFINE_string('model', 'yolo_nas_l', 'yolo_nas_l or yolo_nas_m or yolo_nas_s')\n",
    "flags.DEFINE_string('video', \"16run.mp4\", 'path to input video or set to 0 for webcam')\n",
    "flags.DEFINE_string('output', \"output.mp4\", 'path to output video')\n",
    "flags.DEFINE_float('conf', 0.50, 'confidence threshhold')\n",
    "\n",
    "def main(_argv):\n",
    "    # Initialize the video capture and the video writer objects\n",
    "    video_cap = cv2.VideoCapture(FLAGS.video)\n",
    "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Initialize the video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    writer = cv2.VideoWriter(FLAGS.output, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize the DeepSort tracker\n",
    "    tracker = DeepSort(max_age=50)\n",
    "\n",
    "    # Check if GPU is available, otherwise use CPU\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Load the YOLO model\n",
    "    model = models.get(FLAGS.model, pretrained_weights=\"coco\").to(device)\n",
    "\n",
    "    # Load the COCO class labels the YOLO model was trained on\n",
    "    classes_path = \"C:\\\\Users\\\\obvious\\\\Desktop\\\\sight\\\\coco.names\"\n",
    "    with open(classes_path, \"r\") as f:\n",
    "        class_names = f.read().strip().split(\"\\n\")\n",
    "\n",
    "    # Create a list of random colors to represent each class\n",
    "    np.random.seed(42)  # to get the same colors\n",
    "    colors = np.random.randint(0, 255, size=(len(class_names), 3))  # (80, 3)\n",
    "\n",
    "    while True:\n",
    "        # Start time to compute the FPS\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_cap.read()\n",
    "\n",
    "        # If there is no frame, we have reached the end of the video\n",
    "        if not ret:\n",
    "            print(\"End of the video file...\")\n",
    "            break\n",
    "\n",
    "        # Run the YOLO model on the frame\n",
    "\n",
    "        # Perform object detection using the YOLO model on the current frame\n",
    "        detect = next(iter(model.predict(frame, iou=0.5, conf=FLAGS.conf)))\n",
    "\n",
    "        # Extract the bounding box coordinates, confidence scores, and class labels from the detection results\n",
    "        bboxes_xyxy = torch.from_numpy(detect.prediction.bboxes_xyxy).tolist()\n",
    "        confidence = torch.from_numpy(detect.prediction.confidence).tolist()\n",
    "        labels = torch.from_numpy(detect.prediction.labels).tolist()\n",
    "        # Combine the bounding box coordinates and confidence scores into a single list\n",
    "        concate = [sublist + [element] for sublist, element in zip(bboxes_xyxy, confidence)]\n",
    "        # Combine the concatenated list with the class labels into a final prediction list\n",
    "        final_prediction = [sublist + [element] for sublist, element in zip(concate, labels)]\n",
    "\n",
    "        # Initialize the list of bounding boxes and confidences\n",
    "        results = []\n",
    "\n",
    "        # Loop over the detections\n",
    "        for data in final_prediction:\n",
    "            # Extract the confidence (i.e., probability) associated with the detection\n",
    "            confidence = data[4]\n",
    "\n",
    "            # Filter out weak detections by ensuring the confidence is greater than the minimum confidence\n",
    "            if float(confidence) < FLAGS.conf:\n",
    "                continue\n",
    "\n",
    "            # If the confidence is greater than the minimum confidence, draw the bounding box on the frame\n",
    "            xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n",
    "            class_id = int(data[5])\n",
    "            \n",
    "            # Add the bounding box (x, y, w, h), confidence, and class ID to the results list\n",
    "            results.append([[xmin, ymin, xmax - xmin, ymax - ymin], confidence, class_id])\n",
    "\n",
    "        # Update the tracker with the new detections\n",
    "        tracks = tracker.update_tracks(results, frame=frame)\n",
    "        \n",
    "        # Loop over the tracks\n",
    "        for track in tracks:\n",
    "            # If the track is not confirmed, ignore it\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            # Get the track ID and the bounding box\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            class_id = track.get_det_class()\n",
    "            x1, y1, x2, y2 = int(ltrb[0]), int(ltrb[1]), int(ltrb[2]), int(ltrb[3])\n",
    "            \n",
    "            # Get the color for the class\n",
    "            color = colors[class_id]\n",
    "            B, G, R = int(color[0]), int(color[1]), int(color[2])\n",
    "            \n",
    "            # Create text for track ID and class name\n",
    "            text = str(track_id) + \" - \" + str(class_names[class_id])\n",
    "            \n",
    "            # Draw bounding box and text on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (B, G, R), 2)\n",
    "            cv2.rectangle(frame, (x1 - 1, y1 - 20), (x1 + len(text) * 12, y1), (B, G, R), -1)\n",
    "            cv2.putText(frame, text, (x1 + 5, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        # End time to compute the FPS\n",
    "        end = datetime.datetime.now()\n",
    "        \n",
    "        # Show the time it took to process 1 frame\n",
    "        print(f\"Time to process 1 frame: {(end - start).total_seconds() * 1000:.0f} milliseconds\")\n",
    "        \n",
    "        # Calculate the frames per second and draw it on the frame\n",
    "        fps = f\"FPS: {1 / (end - start).total_seconds():.2f}\"\n",
    "        cv2.putText(frame, fps, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 8)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        # Write the frame to the output video file\n",
    "        writer.write(frame)\n",
    "        \n",
    "        # Check for 'q' key press to exit the loop\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # Release video capture and video writer objects\n",
    "    video_cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    # Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(main)\n",
    "    except SystemExit:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15e938aa",
   "metadata": {},
   "source": [
    "这段代码是一个Python脚本，用于实现目标跟踪和识别。它使用了多个库和模块，包括numpy、datetime、opencv、torch、absl、deep_sort_realtime和super_gradients。\n",
    "\n",
    "代码的功能如下：\n",
    "\n",
    "导入必要的库和模块。\n",
    "\n",
    "定义命令行参数。\n",
    "\n",
    "--model：选择使用的模型，可以是'yolo_nas_l'、'yolo_nas_m'或'yolo_nas_s'。\n",
    "--video：输入视频的路径，可以是本地视频文件路径或摄像头设备的编号（设置为0表示使用摄像头）。\n",
    "--output：输出视频的路径。\n",
    "--conf：置信度阈值。\n",
    "创建DeepSort跟踪器对象，用于目标跟踪。\n",
    "\n",
    "根据选择的模型，加载相应的模型。\n",
    "\n",
    "初始化超级梯度框架的模型训练模块。\n",
    "\n",
    "根据命令行参数创建相应的对象。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91ba6332",
   "metadata": {},
   "source": [
    "首先，通过调用cv2.VideoCapture函数打开输入视频文件，并获取视频的宽度、高度和帧率。然后，使用cv2.VideoWriter函数创建一个视频写入对象，用于将处理后的视频帧写入输出文件。\n",
    "\n",
    "接下来，初始化了一个DeepSort对象，用于目标跟踪。然后，检查GPU是否可用，根据可用情况选择使用GPU还是CPU。\n",
    "\n",
    "使用models.get函数加载指定名称的模型，并使用预训练权重文件\"coco\"进行初始化。然后，加载COCO类标签文件，该文件包含了模型训练时使用的类别名称。\n",
    "\n",
    "接下来，使用np.random.randint函数生成一个随机颜色列表，每个颜色代表一个类别。\n",
    "\n",
    "最后，在主函数中调用以上初始化和配置的代码，并执行后续的目标检测和跟踪等操作。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7db829c",
   "metadata": {},
   "source": [
    "这段代码是一个用于视频对象检测的程序，它使用了YOLO（You Only Look Once）模型。主要流程如下：\n",
    "\n",
    "程序从视频文件中逐帧读取画面。\n",
    "对每一帧画面，使用YOLO模型进行对象检测，得到检测结果。\n",
    "从检测结果中提取边界框坐标、置信度、类别标签等信息。\n",
    "根据置信度过滤掉低信度的检测结果，只保留置信度高于设定阈值的检测结果。\n",
    "对于满足置信度要求的检测结果，在画面上绘制出对应的边界框，并标注出类别标签。\n",
    "循环过程会持续到视频文件结束。这个过程中，程序会计算并显示FPS（每秒帧数）。\n",
    "\n",
    "具体到代码，while True 创建了一个无限循环，不断读取和处理视频帧。start = datetime.datetime.now() 记录了开始处理当前帧的时间，用于计算FPS。\n",
    "\n",
    "ret, frame = video_cap.read() 读取视频的下一帧。if not ret: print(\"End of the video file...\") break 判断是否到达视频末尾，到达则结束循环。\n",
    "\n",
    "model.predict(frame, iou=0.5, conf=FLAGS.conf) 使用YOLO模型对帧进行对象检测。预测结果包含边界框坐标、置信度和类别标签等信息。\n",
    "\n",
    "接下来的代码处理检测结果，包括过滤低信度检测、绘制边界框等操作。\n",
    "\n",
    "最后，video_cap.release() 释放视频捕捉对象，cv2.destroyAllWindows() 关闭所有创建的窗口。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8707be0",
   "metadata": {},
   "source": [
    "这段代码是一个使用OpenCV库的Python程序，它使用YOLO（You Only Look Once）模型进行视频中的对象检测，并使用DeepSort模型进行对象跟踪。\n",
    "\n",
    "主要流程如下：\n",
    "\n",
    "从视频文件中逐帧读取画面。\n",
    "使用YOLO模型对每一帧画面进行对象检测，得到检测结果。\n",
    "过滤掉置信度低于设定阈值的低信度检测结果，只保留置信度高于设定阈值的检测结果。\n",
    "对于满足置信度要求的检测结果，在画面上绘制出对应的边界框，并标注出类别标签。\n",
    "更新跟踪器（tracker）的跟踪轨迹（tracks）。\n",
    "循环遍历跟踪轨迹。对于每个跟踪轨迹，如果它没有被确认，就忽略它。否则，获取跟踪轨迹的ID和边界框，并获取该对象的类别ID。\n",
    "根据类别ID获取对应的颜色，然后在画面上绘制边界框和对应的文本信息。\n",
    "计算处理每一帧画面的时间，并显示出来。\n",
    "计算并显示FPS（每秒帧数）。\n",
    "显示处理后的画面。\n",
    "将处理后的画面写入输出视频文件。\n",
    "如果用户按下“q”键，就退出循环。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cdf49ee",
   "metadata": {},
   "source": [
    "在程序结束时，你释放了视频捕捉对象（video_cap）和视频写入对象（writer），然后关闭了所有OpenCV的窗口。\n",
    "\n",
    "__name__ == '__main__'是一个常见的Python模式，用于确保某个代码块只在脚本被直接运行时执行，而不是在作为模块导入时执行。\n",
    "\n",
    "try/except块用于捕获和处理异常。在你的代码中，SystemExit是一个由sys.exit()或Python解释器退出程序引发的异常。如果程序正常结束，那么这个异常会被捕获并忽略（实际上，它会被传递给程序的父进程或操作系统）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015f07d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'origin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-m jupyter_variableinspector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch(3.8.17)\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2420\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch(3.8.17)\\lib\\site-packages\\IPython\\core\\magics\\execution.py:693\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m opts:\n\u001b[0;32m    692\u001b[0m     modulename \u001b[38;5;241m=\u001b[39m opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 693\u001b[0m     modpath \u001b[38;5;241m=\u001b[39m \u001b[43mfind_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodulename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m is not a valid modulename on sys.path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mmodulename\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch(3.8.17)\\lib\\site-packages\\IPython\\utils\\module_paths.py:60\u001b[0m, in \u001b[0;36mfind_mod\u001b[1;34m(module_name)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mFind module `module_name` on sys.path, and return the path to module `module_name`.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    depending on above conditions.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(module_name)\n\u001b[1;32m---> 60\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmeta_path:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'origin'"
     ]
    }
   ],
   "source": [
    "%run -m jupyter_variableinspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d84fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.8.17",
   "language": "python",
   "name": "pytorch3.8.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
